###############################################################################
#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking
#  Remote GPUç‰ˆæœ¬ - é€šè¿‡HTTPè°ƒç”¨è¿œç¨‹GPUæœåŠ¡
###############################################################################

import math
import torch
import numpy as np

import os
import time
import cv2
import glob
import pickle
import copy
import base64
import requests

import queue
from queue import Queue
from threading import Thread, Event
import torch.multiprocessing as mp

from lipasr import LipASR
import asyncio
from av import AudioFrame, VideoFrame
from basereal import BaseReal

from tqdm import tqdm
from logger import logger

device = "cpu"  # CPUç«¯ä¸éœ€è¦GPU
print('Using CPU with remote GPU inference.')


def load_avatar(avatar_id):
    """åŠ è½½avataræ•°æ®"""
    avatar_path = f"./data/avatars/{avatar_id}"
    full_imgs_path = f"{avatar_path}/full_imgs" 
    face_imgs_path = f"{avatar_path}/face_imgs" 
    coords_path = f"{avatar_path}/coords.pkl"
    
    with open(coords_path, 'rb') as f:
        coord_list_cycle = pickle.load(f)
    input_img_list = glob.glob(os.path.join(full_imgs_path, '*.[jpJP][pnPN]*[gG]'))
    input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
    frame_list_cycle = read_imgs(input_img_list)
    
    input_face_list = glob.glob(os.path.join(face_imgs_path, '*.[jpJP][pnPN]*[gG]'))
    input_face_list = sorted(input_face_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
    face_list_cycle = read_imgs(input_face_list)

    return frame_list_cycle, face_list_cycle, coord_list_cycle


def read_imgs(img_list):
    frames = []
    logger.info('reading images...')
    for img_path in tqdm(img_list):
        frame = cv2.imread(img_path)
        frames.append(frame)
    return frames


def __mirror_index(size, index):
    turn = index // size
    res = index % size
    if turn % 2 == 0:
        return res
    else:
        return size - res - 1 


class RemoteGPUClient:
    """è¿œç¨‹GPUæœåŠ¡å®¢æˆ·ç«¯"""
    def __init__(self, gpu_server_url, session_id, face_list_cycle):
        self.gpu_server_url = gpu_server_url.rstrip('/')
        self.session_id = session_id
        self.face_list_cycle = face_list_cycle
        self.session_initialized = False
        
    def init_session(self):
        """åˆå§‹åŒ–è¿œç¨‹sessionï¼Œä¸Šä¼ faceå›¾ç‰‡"""
        if self.session_initialized:
            return True
            
        try:
            logger.info(f"Initializing remote GPU session {self.session_id}...")
            
            # ç¼–ç faceå›¾ç‰‡ä¸ºbase64
            face_imgs_b64 = []
            for face in self.face_list_cycle:
                _, buffer = cv2.imencode('.png', face)
                face_b64 = base64.b64encode(buffer).decode('utf-8')
                face_imgs_b64.append(face_b64)
            
            # å‘é€åˆå§‹åŒ–è¯·æ±‚
            resp = requests.post(
                f"{self.gpu_server_url}/session/init",
                json={
                    'session_id': self.session_id,
                    'face_imgs': face_imgs_b64
                },
                timeout=60
            )
            
            if resp.status_code == 200:
                result = resp.json()
                logger.info(f"Remote session initialized: {result}")
                self.session_initialized = True
                return True
            else:
                logger.error(f"Failed to init session: {resp.status_code} {resp.text}")
                return False
                
        except Exception as e:
            logger.exception("Error initializing remote session")
            return False
    
    def inference_batch(self, mel_batch, face_indices):
        """è°ƒç”¨è¿œç¨‹æ¨ç†ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ‰¹é‡ä¼ è¾“ï¼‰"""
        try:
            if not self.session_initialized:
                if not self.init_session():
                    raise Exception("Failed to initialize remote session")
            
            # ç¼–ç mel_batchï¼ˆä½¿ç”¨æ›´é«˜æ•ˆçš„float16å‡å°‘ä¼ è¾“é‡ï¼‰
            mel_float16 = mel_batch.astype(np.float16)  # å‡åŠæ•°æ®é‡
            mel_bytes = mel_float16.tobytes()
            mel_b64 = base64.b64encode(mel_bytes).decode('utf-8')
            
            # å‘é€æ¨ç†è¯·æ±‚
            resp = requests.post(
                f"{self.gpu_server_url}/inference/batch",
                json={
                    'session_id': self.session_id,
                    'mel_batch': mel_b64,
                    'mel_shape': list(mel_batch.shape),
                    'mel_dtype': 'float16',  # æ ‡è®°æ•°æ®ç±»å‹
                    'face_indices': face_indices
                },
                timeout=10
            )
            
            if resp.status_code == 200:
                result = resp.json()
                
                # ä¼˜åŒ–ï¼šæ‰¹é‡è§£ç 
                if 'batch_data' in result:
                    # æ–°ç‰ˆæœ¬ï¼šæ‰¹é‡ä¼ è¾“
                    batch_bytes = base64.b64decode(result['batch_data'])
                    batch_shape = tuple(result['batch_shape'])
                    frames = np.frombuffer(batch_bytes, dtype=np.uint8).reshape(batch_shape)
                    logger.info(f"âœ“ Remote inference OK: batch_shape={batch_shape}, fps={result.get('fps', 0):.1f}")
                    return frames.astype(np.float32)
                else:
                    # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šé€å¸§ä¼ è¾“
                    frames = []
                    for frame_b64 in result['frames']:
                        frame_bytes = base64.b64decode(frame_b64)
                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)
                        frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)
                        frames.append(frame.astype(np.float32))
                    logger.info(f"âœ“ Remote inference OK: {len(frames)} frames")
                    return np.array(frames)
            else:
                logger.error(f"Remote inference failed: {resp.status_code} {resp.text}")
                return None
                
        except Exception as e:
            logger.exception("Error in remote inference")
            return None
    
    def close_session(self):
        """å…³é—­è¿œç¨‹session"""
        if not self.session_initialized:
            return
            
        try:
            resp = requests.post(
                f"{self.gpu_server_url}/session/close",
                json={'session_id': self.session_id},
                timeout=5
            )
            logger.info(f"Remote session closed: {resp.status_code}")
        except Exception as e:
            logger.warning(f"Error closing remote session: {e}")


def inference(quit_event, batch_size, face_list_cycle, audio_feat_queue, audio_out_queue, 
              res_frame_queue, gpu_client):
    """
    æ¨ç†çº¿ç¨‹ - è°ƒç”¨è¿œç¨‹GPUæœåŠ¡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    """
    length = len(face_list_cycle)
    index = 0
    count = 0
    counttime = 0
    logger.info('start remote inference')
    
    while not quit_event.is_set():
        mel_batch = []
        try:
            mel_batch = audio_feat_queue.get(block=True, timeout=1)
        except queue.Empty:
            continue
            
        is_all_silence = True
        audio_frames = []
        for _ in range(batch_size*2):
            frame, type, eventpoint = audio_out_queue.get()
            audio_frames.append((frame, type, eventpoint))
            if type == 0:
                is_all_silence = False

        if is_all_silence:
            for i in range(batch_size):
                res_frame_queue.put((None, __mirror_index(length, index), audio_frames[i*2:i*2+2]))
                index = index + 1
        else:
            logger.debug(f"ğŸ¤ Speech detected, calling remote inference (batch_size={batch_size})")
            t = time.perf_counter()
            
            # ã€ä¼˜åŒ–ã€‘é¢„è®¡ç®—faceç´¢å¼•ï¼Œé¿å…é‡å¤è®¡ç®—
            face_indices = [__mirror_index(length, index + i) for i in range(batch_size)]
            
            # ã€ä¼˜åŒ–ã€‘ç›´æ¥ä½¿ç”¨mel_batchï¼ˆå·²ç»æ˜¯numpyæ•°ç»„ï¼‰ï¼Œé¿å…äºŒæ¬¡è½¬æ¢
            # mel_batchæœ¬èº«å·²ç»æ˜¯list of numpyï¼Œç›´æ¥è½¬ä¸ºnumpyæ•°ç»„
            if isinstance(mel_batch, list):
                mel_batch_np = np.stack(mel_batch, axis=0) if len(mel_batch) > 0 else np.array(mel_batch)
            else:
                mel_batch_np = mel_batch
            
            # è°ƒç”¨è¿œç¨‹GPUæ¨ç†
            pred = gpu_client.inference_batch(mel_batch_np, face_indices)
            
            if pred is None:
                logger.error("Remote inference failed, using silence frames")
                for i in range(batch_size):
                    res_frame_queue.put((None, __mirror_index(length, index), audio_frames[i*2:i*2+2]))
                    index = index + 1
                continue
            
            infer_time = time.perf_counter() - t
            counttime += infer_time
            count += batch_size
            
            if count >= 100:
                avg_fps = count / counttime
                logger.info(f"------actual avg final fps:{avg_fps:.4f}")
                count = 0
                counttime = 0
                
            # ã€ä¼˜åŒ–ã€‘ç›´æ¥éå†ï¼Œé¿å…enumerateå¼€é”€
            logger.debug(f"ğŸ“¹ Putting {len(pred)} frames to queue, pred shape={pred.shape}, dtype={pred.dtype}, value_range=[{pred.min():.1f}, {pred.max():.1f}]")
            for i in range(len(pred)):
                res_frame_queue.put((pred[i], __mirror_index(length, index), audio_frames[i*2:i*2+2]))
                index = index + 1
                
    logger.info('lipreal remote inference processor stop')


class LipReal(BaseReal):
    @torch.no_grad()
    def __init__(self, opt, model, avatar):
        super().__init__(opt)
        
        self.fps = opt.fps
        self.batch_size = opt.batch_size
        self.idx = 0
        self.res_frame_queue = Queue(self.batch_size*2)
        
        # avataræ•°æ®
        self.frame_list_cycle, self.face_list_cycle, self.coord_list_cycle = avatar
        
        # åˆ›å»ºè¿œç¨‹GPUå®¢æˆ·ç«¯
        self.gpu_client = RemoteGPUClient(
            gpu_server_url=opt.gpu_server_url,
            session_id=opt.sessionid,
            face_list_cycle=self.face_list_cycle
        )
        
        self.asr = LipASR(opt, self)
        self.asr.warm_up()
        
        self.render_event = mp.Event()

    def paste_back_frame(self, pred_frame, idx: int):
        bbox = self.coord_list_cycle[idx]
        combine_frame = copy.deepcopy(self.frame_list_cycle[idx])
        y1, y2, x1, x2 = bbox
        res_frame = cv2.resize(pred_frame.astype(np.uint8), (x2-x1, y2-y1))
        combine_frame[y1:y2, x1:x2] = res_frame
        return combine_frame
            
    def render(self, quit_event, loop=None, audio_track=None, video_track=None):
        self.init_customindex()
        self.tts.render(quit_event)
        
        # å¯åŠ¨æ¨ç†çº¿ç¨‹
        infer_quit_event = Event()
        infer_thread = Thread(
            target=inference, 
            args=(infer_quit_event, self.batch_size, self.face_list_cycle,
                  self.asr.feat_queue, self.asr.output_queue, self.res_frame_queue,
                  self.gpu_client,)
        )
        infer_thread.start()
        
        # å¯åŠ¨å¸§å¤„ç†çº¿ç¨‹
        process_quit_event = Event()
        process_thread = Thread(target=self.process_frames, args=(process_quit_event, loop, audio_track, video_track))
        process_thread.start()

        count = 0
        totaltime = 0
        _starttime = time.perf_counter()
        
        while not quit_event.is_set(): 
            t = time.perf_counter()
            self.asr.run_step()

            if video_track and video_track._queue.qsize() >= 5:
                logger.debug('sleep qsize=%d', video_track._queue.qsize())
                time.sleep(0.04 * video_track._queue.qsize() * 0.8)
                
        logger.info('lipreal thread stop')

        # åœæ­¢æ¨ç†çº¿ç¨‹
        infer_quit_event.set()
        infer_thread.join()

        # åœæ­¢å¸§å¤„ç†çº¿ç¨‹
        process_quit_event.set()
        process_thread.join()
        
        # å…³é—­è¿œç¨‹session
        self.gpu_client.close_session()
